{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AppDev Part One: Timeseries Data from Predix\n",
    "by: Alireza Dibazar\n",
    "dibazar@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "We will learn to develope an app for live streaming of IoT data. The app is suppose to provide analytics-based anomaly detection with details needed to guide field engineers. The data is fetched from Predix Timeseries database and it provides readings of a Three-axis accelerometer which installed in a gas turbine. It is suggested to see the deployed version of the app before the exercise. The app can be found at the following url: https://sample-live-streaming.run.aws-usw02-pr.ice.predix.io/ and [document here](./Documents/Sample_live_Stream.pptx) (Sample_live_Stream.pptx)\n",
    "\n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Learn planning of the project based of agile methodology\n",
    "2. Learn how to read data from Cassandra -- Predix Timeseries database -- and be able to change sampling frequency of the data\n",
    "3. Be able to visually demonstrate field engineers pain point and convert that to a wireframe\n",
    "4. Design analytics\n",
    "5. Learn to design app layout based on wireframe\n",
    "6. Be able to deply the app and operationalize it for users\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### This notebook \n",
    "THis notebook is focused on reading data from Predix TimeSeries database. Though content of this notebook will be used for live streaming of timeseries, here students employ this code to access historical data for offline data analysis and model building.\n",
    "\n",
    "please note that the historical data which is stored in the Predix, does not include any anomalies and therefore students are encoraged to consider models such as PCA, T2-Hotelling, similarity based models.\n",
    "\n",
    "\n",
    "### Assignment\n",
    "1. Study Predix Timeseries website: https://docs.predix.io/en-US/content/service/data_management/time_series/\n",
    "2. Creat three data frame and read data from the database interval at three frequencies \"interpolation\": one, 10, and 60 seconds; plot and visually inspect differences\n",
    "3. Read and store data in your local computer and build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This section imports necessary packages \n",
    "\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "# import http.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tag names of the intrest are: 'Feather2.GX', 'Feather2.GY', and 'Feather2.GZ'\n",
    "# \n",
    "Tags = ['Feather2.GX', 'Feather2.GY', 'Feather2.GZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read data from Predix Timeseries database\n",
    "Predix Timeseries database has been built on Cassandra. For the purpose of this course we have made an API call to access the data and bring it to working environment.\n",
    "\n",
    "1. We need an access token; the token expires every \"expiration_time\" which is about 24hrs\n",
    "2. The token and expiration time is stored in a file \"Data/token_expiration_time.txt\"\n",
    "3. When requesting new data from database we use the available token unless it is expired\n",
    "4. For this project Three tags are read from the database. They are namely 'Feather2.GX', 'Feather2.GY', 'Feather2.GZ'\n",
    "5. For a single read we need to define start_time and end_time. These times have to be UTC and in milisecond\n",
    "6. We can dynamically change \"aggregations\". In this practice it is set to be 60 second. \n",
    "\n",
    "More information can be found in https://docs.predix.io/en-US/content/service/data_management/time_series/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def payload_json(start_time, end_time, Tags, interval):\n",
    "    '''\n",
    "    \n",
    "    Takes three inputs parameters and creates a dictionary in json format. The Three inputs are:\n",
    "    a) start time of data, b) end time of data, and c) Tag names for which data are pulled.\n",
    "    The output is a JSON string of the reqest which will be used\n",
    "    when we send the request to the database: requests.request(...,...,data = payload_json(m, n, Tags), ...)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    q = {\n",
    "#       \"cache_time\": 0,\n",
    "      \"tags\": [\n",
    "        {\n",
    "          \"name\": Tags,\n",
    "          \"aggregations\": [{\"type\": \"interpolate\", \"interval\": interval}],\n",
    "          \"order\": \"asc\"\n",
    "        }\n",
    "      ],\n",
    "      \"start\": start_time,\n",
    "      \"end\": end_time\n",
    "    }\n",
    "#     print(json.dumps(q))\n",
    "    return json.dumps(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tidy_df_from_jsondict(json_dict):\n",
    "    ''' \n",
    "    \n",
    "    Extract data from JSON string and stores in a dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    times, tags, values = [], [], []\n",
    "    \n",
    "    for tag_dict in json_dict['tags']:\n",
    "\n",
    "        val_list = tag_dict['results'][0]['values']\n",
    "        \n",
    "        for v in val_list:\n",
    "            times.append(v[0])\n",
    "            tags.append(tag_dict['name'])\n",
    "            values.append(v[1])\n",
    "\n",
    "    df = pd.DataFrame({'time':times, 'tag':tags, 'value': values})\n",
    "    \n",
    "    df['value'] = df['value'].astype(np.float, copy=True, errors='ignore')\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_token():\n",
    "    '''\n",
    "    Function to get Authorization token.\n",
    "    Students are aksed to read details from Predix Timeseries website\n",
    "\n",
    "    '''\n",
    "    \n",
    "    url = \"https://d1e53858-2903-4c21-86c0-95edc7a5cef2.predix-uaa.run.aws-usw02-pr.ice.predix.io/oauth/token\"\n",
    "\n",
    "    payload = \"grant_type=client_credentials\"\n",
    "    headers = {\n",
    "        'Content-Type': \"application/x-www-form-urlencoded\",\n",
    "        'Authorization': \"Basic cHJlZGl4YXZlbmdlcnNzYl90czpZVzlLYVNIYXRoRTVibTh2RzhLRnlmWUY=\",\n",
    "        'User-Agent': \"PostmanRuntime/7.13.0\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Postman-Token': \"fe4920c7-5519-486d-bd08-8c42dfd712d1,a9da8cf0-f8db-4955-82e4-44de44058f96\",\n",
    "        'Host': \"d1e53858-2903-4c21-86c0-95edc7a5cef2.predix-uaa.run.aws-usw02-pr.ice.predix.io\",\n",
    "        'accept-encoding': \"gzip, deflate\",\n",
    "        'content-length': \"29\",\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "        }\n",
    "    response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "    data = response.text\n",
    "#     print(response.text)\n",
    "\n",
    "    json_dict = json.loads(data)\n",
    "    token = 'Bearer '+json_dict['access_token']\n",
    "    expires_in = json_dict['expires_in']\n",
    "#     print(data.decode(\"utf-8\"))\n",
    "\n",
    "    utc_tm = int(dt.datetime.utcnow().timestamp())\n",
    "    expiration_time = utc_tm + expires_in\n",
    "    \n",
    "    fid = open(\"Data/token_expiration_time.txt\",\"w\")\n",
    "    fid.write(str(expiration_time)+'\\n')\n",
    "    fid.write(token)\n",
    "    fid.close()\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_from_timeseries_database(m,n,Tags,authorization, interval):\n",
    "    '''\n",
    "    # This function fetches data from time m to time n for tag names specified in varibale named \"Tags\"\n",
    "    '''\n",
    "    \n",
    "    print('This is get_data_from_timeseries_database()','\\n')\n",
    "    url = \"https://time-series-store-predix.run.aws-usw02-pr.ice.predix.io/v1/datapoints/\"\n",
    "    interval = interval\n",
    "    payload = payload_json(m,n,Tags, interval)\n",
    "    headers = {\n",
    "        'Content-Type': \"application/json\",\n",
    "        'Authorization': authorization,\n",
    "        'Predix-Zone-Id': \"38357f8f-2ca8-4b67-9479-2a0748c8becd\",\n",
    "        'User-Agent': \"PostmanRuntime/7.13.0\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Postman-Token': \"d3cf433e-75cd-4d85-a671-ec2b3b5daf0c,db1e50b3-bcb2-4723-bed2-96a182e5bb8e\",\n",
    "        'Host': \"time-series-store-predix.run.aws-usw02-pr.ice.predix.io\",\n",
    "        'accept-encoding': \"gzip, deflate\",\n",
    "        'content-length': \"148\",\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "        }\n",
    "\n",
    "    response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "\n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE: Create a function to read a block of data from 2019-07-17 to 2019-07-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_some_data(start, end, interval):\n",
    "    \n",
    "#     read token and if it is expired request a new token\n",
    "    fid = open(\"Data/token_expiration_time.txt\",\"r\")\n",
    "    utc_tm = int(dt.datetime.utcnow().timestamp())\n",
    "    expiration_time = fid.readline()\n",
    "    authorization = fid.readline()\n",
    "    fid.close()\n",
    "    \n",
    "    print('Token Expired: ', (int(expiration_time)- utc_tm)<=0 ,'\\n')\n",
    "    if (int(expiration_time) <= utc_tm):\n",
    "        authorization = get_token()\n",
    "#         print(authorization,'\\n')\n",
    "    \n",
    "#     convert timestring to UTC\n",
    "    m = dt.datetime.strptime(start, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "    n = dt.datetime.strptime(end, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "    M = m*1000                  # epoch in milisec\n",
    "    N = n*1000                  # epoch in milisec\n",
    "    interval = '1s'\n",
    "        \n",
    "    data = get_data_from_timeseries_database(M,N,Tags,authorization, interval)\n",
    "\n",
    "    json_data = json.loads(data)\n",
    "\n",
    "    _df = create_tidy_df_from_jsondict(json_data)\n",
    "    _df = _df.drop_duplicates()\n",
    "    df = _df.pivot(index='time', columns='tag', values='value')\n",
    "\n",
    "    for c in Tags:\n",
    "        if c not in df.columns:\n",
    "            df[c] =np.nan\n",
    "\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    \n",
    "    for c in cols:\n",
    "        new_cols.append(c.split('.')[-1])\n",
    "        \n",
    "    df.columns = new_cols\n",
    "    df = df[sorted(df.columns)]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = get_some_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-07-17 00:00:00\n",
    "2019-07-18 11:59:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data in local hard drive for off-line analysis and model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and store data in your local computer and build your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = '2019-07-17 00:00:00'\n",
    "end = '2019-07-18 11:59:59'\n",
    "interval = '1s'\n",
    "df_1s = get_some_data(start, end, interval)\n",
    "pd.write_csv(df_1s, 'sample_data_1s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = '2019-07-17 00:00:00'\n",
    "end = '2019-07-30 11:59:59'\n",
    "interval = '10s'\n",
    "df_10s = get_some_data(start, end, interval)\n",
    "pd.write_csv(df_10s, 'sample_data_10s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Expired:  False \n",
      "\n",
      "This is get_data_from_timeseries_database() \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = '2019-09-23 00:00:00'\n",
    "end = '2019-09-23 11:59:59'\n",
    "interval = '60s'\n",
    "df_60s = get_some_data(start, end, interval)\n",
    "df_60s.to_csv('data_60s', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
